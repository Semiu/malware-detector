{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TargetEncoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqsug_mwtjRT"
      },
      "source": [
        "Applying Target Encoding, as a feature engineering technique, to attempt to improve the model performances. Three out of five models developed at the basic modeling stage are transitioned to this stage. The models are Neural Network, Logistic Regression and XGBoost.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcawNeg-ti3w"
      },
      "source": [
        "#Import the libraries \r\n",
        "import copy\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from tqdm import tqdm\r\n",
        "from sklearn import ensemble\r\n",
        "from sklearn import linear_model, metrics, preprocessing\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQHM0-Uo4jQe"
      },
      "source": [
        "#Load the data - The datsset used for the modeling, the fractional data, is loaded directly\r\n",
        "modeling_dataset = pd.read_csv('/content/drive/MyDrive/prediction/frac_cleaned_fod_data.csv', low_memory = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYprm4Va6Ku-"
      },
      "source": [
        "#All columns except 'HasDetections', 'kfold', and 'MachineIdentifier' as training features\r\n",
        "train_features = [tf for tf in modeling_dataset.columns if tf not in ('HasDetections', 'kfold', 'MachineIdentifier')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "865BF1qo4rwk"
      },
      "source": [
        "#Define the categorical features of the data\r\n",
        "categorical_features = ['ProductName',\r\n",
        "                        'EngineVersion',\r\n",
        "                        'AppVersion',\r\n",
        "                        'AvSigVersion',\r\n",
        "                        'Platform',\r\n",
        "                        'Processor',\r\n",
        "                        'OsVer',\r\n",
        "                        'OsPlatformSubRelease',\r\n",
        "                        'OsBuildLab',\r\n",
        "                        'SkuEdition',\r\n",
        "                        'Census_MDC2FormFactor',\r\n",
        "                        'Census_DeviceFamily',\r\n",
        "                        'Census_PrimaryDiskTypeName',\r\n",
        "                        'Census_ChassisTypeName',\r\n",
        "                        'Census_PowerPlatformRoleName',\r\n",
        "                        'Census_OSVersion',\r\n",
        "                        'Census_OSArchitecture',\r\n",
        "                        'Census_OSBranch',\r\n",
        "                        'Census_OSEdition',\r\n",
        "                        'Census_OSSkuName',\r\n",
        "                        'Census_OSInstallTypeName',\r\n",
        "                        'Census_OSWUAutoUpdateOptionsName',\r\n",
        "                        'Census_GenuineStateName',\r\n",
        "                        'Census_ActivationChannel',\r\n",
        "                        'Census_FlightRing']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yWiNCQ0ti0y"
      },
      "source": [
        "#Mean Target encoding function for Tree models\r\n",
        "def mean_target_encoding_tree(data):\r\n",
        "  \"\"\"\r\n",
        "  The function takes the data frame the data to be used for the modeling\r\n",
        "  to return one with encoded data\r\n",
        "  \"\"\"\r\n",
        "  #Make a copy of the data frame\r\n",
        "  data_copy = copy.deepcopy(data)\r\n",
        "\r\n",
        "  #Label encoding the categorical features\r\n",
        "  for col in train_features:\r\n",
        "    if col in categorical_features:\r\n",
        "      #Initialize Label Encoder for each of the categorical features\r\n",
        "      lbl = preprocessing.LabelEncoder()\r\n",
        "      #Fit the label encoder\r\n",
        "      lbl.fit(data_copy[col])\r\n",
        "      #Transform the data\r\n",
        "      data_copy.loc[:,col] = lbl.transform(data_copy[col])\r\n",
        "  \r\n",
        "  #A list to store 10 validation dataframes\r\n",
        "  encoded_data_copies = []\r\n",
        "\r\n",
        "  #Over the folds\r\n",
        "  for fold in range(10):\r\n",
        "    #Get training and validation data using folds\r\n",
        "    data_copy_train = data_copy[data_copy.kfold != fold].reset_index(drop=True)\r\n",
        "    data_copy_valid = data_copy[data_copy.kfold == fold].reset_index(drop=True)\r\n",
        "    #for the categorical features\r\n",
        "    for column in categorical_features:\r\n",
        "      #create dictionary of category for mean target\r\n",
        "      mapping_dict = dict(\r\n",
        "          data_copy_train.groupby(column)['HasDetections'].mean()\r\n",
        "      )\r\n",
        "      #A new column with the mean encoding to be named\r\n",
        "      data_copy_valid.loc[:, column+\"_enc\"] = data_copy_valid[column].map(mapping_dict)\r\n",
        "    #Append to the list of encoded validation data frames\r\n",
        "    encoded_data_copies.append(data_copy_valid)\r\n",
        "  #Create the encoded full data frame and return it\r\n",
        "  encoded_data_copy = pd.concat(encoded_data_copies, axis=0)\r\n",
        "  return encoded_data_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz4uRpWCo_DS"
      },
      "source": [
        "#One hot encoding the features\r\n",
        "  for col in train_features:\r\n",
        "    #Initialize one hot encoding\r\n",
        "    ohe = preprocessing.OneHotEncoder()\r\n",
        "    #Fit the One hot encoder\r\n",
        "    ohe.fit(data_copy[col])\r\n",
        "    #Transform the data\r\n",
        "    data_copy.loc[:,col] = ohe.transform(data_copy[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6ODPezG6ivO"
      },
      "source": [
        "#Mean Target encoding function for Linear models\r\n",
        "def mean_target_encoding_linear(data):\r\n",
        "  #Make a copy of the data frame\r\n",
        "  data_copy = copy.deepcopy(data)\r\n",
        "  \r\n",
        "  #A list to store 10 validation dataframes\r\n",
        "  encoded_data_copies = []\r\n",
        "\r\n",
        "  #Over the folds\r\n",
        "  for fold in range(10):\r\n",
        "    #Get training and validation data using folds\r\n",
        "    data_copy_train = data_copy[data_copy.kfold != fold].reset_index(drop=True)\r\n",
        "    data_copy_valid = data_copy[data_copy.kfold == fold].reset_index(drop=True)\r\n",
        "    #for the features\r\n",
        "    for column in train_features:\r\n",
        "      #create dictionary for the mean target\r\n",
        "      mapping_dict = dict(\r\n",
        "          data_copy_train.groupby(column)['HasDetections'].mean()\r\n",
        "      )\r\n",
        "      #A new column with the mean encoding to be named\r\n",
        "      data_copy_valid.loc[:, column+\"_enc\"] = data_copy_valid[column].map(mapping_dict)\r\n",
        "    #Append to the list of encoded validation data frames\r\n",
        "    encoded_data_copies.append(data_copy_valid)\r\n",
        "  #Create the encoded full data frame and return it\r\n",
        "  encoded_data_copy = pd.concat(encoded_data_copies, axis=0)\r\n",
        "  return encoded_data_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMIaLvsAtix7"
      },
      "source": [
        "#XGBoost\r\n",
        "def run_xgb(df, fold):\r\n",
        "  #Get training and validation data using folds\r\n",
        "  modeling_datasets_train = df[df.kfold != fold].reset_index(drop=True)\r\n",
        "  modeling_datasets_valid = df[df.kfold == fold].reset_index(drop=True)\r\n",
        "\r\n",
        "  #Scale the data\r\n",
        "  X_train = modeling_datasets_train[train_features].values\r\n",
        "  X_valid = modeling_datasets_valid[train_features].values\r\n",
        "\r\n",
        "  #Initialize XGboost model\r\n",
        "  xgb_model = xgb.XGBClassifier(n_jobs=-1)\r\n",
        "  \r\n",
        "  #Fit the model on training data\r\n",
        "  xgb_model.fit(X_train, modeling_datasets_train.HasDetections.values)\r\n",
        "\r\n",
        "\r\n",
        "  #Predict on validation\r\n",
        "  valid_preds = xgb_model.predict(X_valid)\r\n",
        "\r\n",
        "  #Get the ROC AUC score\r\n",
        "  auc = metrics.roc_auc_score(modeling_datasets_valid.HasDetections.values, valid_preds)\r\n",
        "\r\n",
        "  #Get the precision score\r\n",
        "  pre = metrics.precision_score(modeling_datasets_valid.HasDetections.values, valid_preds, average='binary')\r\n",
        "\r\n",
        "  #Get the Recall score\r\n",
        "  rc = metrics.recall_score(modeling_datasets_valid.HasDetections.values, valid_preds, average='binary')\r\n",
        "\r\n",
        "  return auc, pre, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5g5bjam6Gen"
      },
      "source": [
        "#Neural Network\r\n",
        "def run_nn(df, fold):\r\n",
        "  #Get training and validation data using folds\r\n",
        "  modeling_datasets_train = df[df.kfold != fold].reset_index(drop=True)\r\n",
        "  modeling_datasets_valid = df[df.kfold == fold].reset_index(drop=True)\r\n",
        "\r\n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\r\n",
        "  ohe = preprocessing.OneHotEncoder()\r\n",
        "  full_data = pd.concat(\r\n",
        "    [modeling_datasets_train[train_features],modeling_datasets_valid[train_features]],\r\n",
        "    axis = 0\r\n",
        "    )\r\n",
        "  ohe.fit(full_data[train_features])\r\n",
        "\r\n",
        "  #Scale the data\r\n",
        "  X_train = ohe.transform(modeling_datasets_train[train_features])\r\n",
        "  X_valid = ohe.transform(modeling_datasets_valid[train_features])\r\n",
        "\r\n",
        "  #Initialize the Neural Network Model\r\n",
        "  nn_model = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\r\n",
        "  \r\n",
        "  #Fit the model on training data\r\n",
        "  nn_model.fit(X_train, modeling_datasets_train.HasDetections.values)\r\n",
        "\r\n",
        "  #Predict on validation\r\n",
        "  valid_preds = nn_model.predict_proba(X_valid)[:,1]\r\n",
        "  valid_preds_pc = nn_model.predict(X_valid)\r\n",
        "\r\n",
        "  #Get the ROC AUC score\r\n",
        "  auc = metrics.roc_auc_score(modeling_datasets_valid.HasDetections.values, valid_preds)\r\n",
        "\r\n",
        "  #Get the precision score\r\n",
        "  pre = metrics.precision_score(modeling_datasets_valid.HasDetections.values, valid_preds_pc, average='binary')\r\n",
        "\r\n",
        "  #Get the Recall score\r\n",
        "  rc = metrics.recall_score(modeling_datasets_valid.HasDetections.values, valid_preds_pc, average='binary')\r\n",
        "\r\n",
        "  return auc, pre, rc\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEP4IhPr6KVM"
      },
      "source": [
        "#Linear Regression\r\n",
        "def run_lr(df, fold):\r\n",
        "  #Get training and validation data using folds\r\n",
        "  modeling_datasets_train = df[df.kfold != fold].reset_index(drop=True)\r\n",
        "  modeling_datasets_valid = df[df.kfold == fold].reset_index(drop=True)\r\n",
        "\r\n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\r\n",
        "  ohe = preprocessing.OneHotEncoder()\r\n",
        "  full_data = pd.concat(\r\n",
        "    [modeling_datasets_train[train_features],modeling_datasets_valid[train_features]],\r\n",
        "    axis = 0\r\n",
        "    )\r\n",
        "  ohe.fit(full_data[train_features])\r\n",
        "\r\n",
        "  #Scale the data\r\n",
        "  X_train = ohe.transform(modeling_datasets_train[train_features])\r\n",
        "  X_valid = ohe.transform(modeling_datasets_valid[train_features])\r\n",
        "\r\n",
        "  #Initialize the Logistic Regression Model\r\n",
        "  lr_model = linear_model.LogisticRegression()\r\n",
        "  \r\n",
        "  #Fit the model on training data\r\n",
        "  lr_model.fit(X_train, modeling_datasets_train.HasDetections.values)\r\n",
        "\r\n",
        "\r\n",
        "  #Predict on validation\r\n",
        "  valid_preds = lr_model.predict_proba(X_valid)[:,1]\r\n",
        "\r\n",
        "  valid_preds_others = lr_model.predict(X_valid)\r\n",
        "\r\n",
        "  #Get the ROC AUC score\r\n",
        "  auc = metrics.roc_auc_score(modeling_datasets_valid.HasDetections.values, valid_preds)\r\n",
        "\r\n",
        "  #Get the precision score\r\n",
        "  pre = metrics.precision_score(modeling_datasets_valid.HasDetections.values, valid_preds_others, average='binary')\r\n",
        "\r\n",
        "  #Get the Recall score\r\n",
        "  rc = metrics.recall_score(modeling_datasets_valid.HasDetections.values, valid_preds_others, average='binary')\r\n",
        "\r\n",
        "  return auc, pre, rc\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTVDcYaB7a_E"
      },
      "source": [
        "#Create encoded data for Linear model\r\n",
        "encoded_data_ln = mean_target_encoding_linear(modeling_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zQwH7nL-XgT"
      },
      "source": [
        "#Save the encoded data (Tree) for future use\r\n",
        "encoded_data_ln.to_csv('/content/drive/MyDrive/prediction/encoded_data_ln.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNu2Hoh4GTbE"
      },
      "source": [
        "#Create the mean target encoded categories and munge data for Tree model\r\n",
        "#This takes the dataframe as argument\r\n",
        "encoded_data_tr = mean_target_encoding_tree(modeling_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6p4wcN--XAL"
      },
      "source": [
        "#Save the encoded data (Tree) for future use\r\n",
        "encoded_data_tr.to_csv('/content/drive/MyDrive/prediction/encoded_data_tr.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnAQ4aA3HXL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f47b02-80e9-4ff4-8582-3a347dfd720c"
      },
      "source": [
        "xgb_list = []\r\n",
        "for fold_ in tqdm(range(10)):\r\n",
        "  xgb_list.append(run_xgb(encoded_data_tr, fold_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:07<00:00, 60.74s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZJmKU-y5m49",
        "outputId": "f667318e-0b4a-4a02-cb44-7fb8944f3c5b"
      },
      "source": [
        "nn_list = []\r\n",
        "for fold_ in tqdm(range(10)):\r\n",
        "  nn_list.append(run_nn(encoded_data_ln, fold_))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 10%|█         | 1/10 [00:27<04:03, 27.01s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 20%|██        | 2/10 [00:55<03:40, 27.56s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 40%|████      | 4/10 [01:41<02:31, 25.18s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 70%|███████   | 7/10 [02:49<01:10, 23.56s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 80%|████████  | 8/10 [03:12<00:46, 23.31s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 90%|█████████ | 9/10 [03:35<00:23, 23.14s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 10/10 [03:58<00:00, 23.81s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUlFYiHO5m11",
        "outputId": "d5fef227-b4bf-4b8a-e8c7-ca8d6191e3f1"
      },
      "source": [
        "lr_list = []\r\n",
        "for fold_ in tqdm(range(10)):\r\n",
        "  lr_list.append(run_lr(encoded_data_ln, fold_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 10%|█         | 1/10 [01:19<11:51, 79.01s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 20%|██        | 2/10 [02:36<10:28, 78.57s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 30%|███       | 3/10 [03:57<09:15, 79.29s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 40%|████      | 4/10 [05:14<07:52, 78.73s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 50%|█████     | 5/10 [06:31<06:30, 78.09s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 60%|██████    | 6/10 [07:48<05:10, 77.73s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 70%|███████   | 7/10 [09:05<03:52, 77.41s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 80%|████████  | 8/10 [10:22<02:34, 77.49s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 90%|█████████ | 9/10 [11:39<01:17, 77.17s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "100%|██████████| 10/10 [12:55<00:00, 77.55s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVvSGqWb5my5"
      },
      "source": [
        "xgb_auc = []\r\n",
        "xgb_pre = []\r\n",
        "xgb_rc = []\r\n",
        "\r\n",
        "for i in xgb_list:\r\n",
        "  xgb_auc.append(i[0])\r\n",
        "  xgb_pre.append(i[1])\r\n",
        "  xgb_rc.append(i[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuuCxIvI5mlo"
      },
      "source": [
        "nn_auc = []\r\n",
        "nn_pre = []\r\n",
        "nn_rc = []\r\n",
        "\r\n",
        "for j in nn_list:\r\n",
        "  nn_auc.append(j[0])\r\n",
        "  nn_pre.append(j[1])\r\n",
        "  nn_rc.append(j[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3MoVvCQ5miw"
      },
      "source": [
        "lr_auc = []\r\n",
        "lr_pre = []\r\n",
        "lr_rc = []\r\n",
        "\r\n",
        "for k in lr_list:\r\n",
        "  lr_auc.append(k[0])\r\n",
        "  lr_pre.append(k[1])\r\n",
        "  lr_rc.append(k[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npBfVNbn5mf4"
      },
      "source": [
        "#Dictionary to hold the target encoding data\r\n",
        "target_encoding_performance = {\"logistic_regression\": {\"auc\":\"\", \"precision\":\"\", \"recall\":\"\"},\r\n",
        "                           \"xgboost\": {\"auc\":\"\",\"precision\":\"\",\"recall\":\"\"},\r\n",
        "                           \"nn\":{\"auc\":\"\",\"precision\":\"\",\"recall\":\"\"}\r\n",
        "                           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AJ9e6o15mdA"
      },
      "source": [
        "#Calculate average of each of the lists of performance metrics and update the dictionary\r\n",
        "target_encoding_performance['logistic_regression'].update({'auc':sum(lr_auc)/len(lr_auc)})\r\n",
        "target_encoding_performance['xgboost'].update({'auc':sum(xgb_auc)/len(xgb_auc)})\r\n",
        "target_encoding_performance['nn'].update({'auc':sum(nn_auc)/len(nn_auc)})\r\n",
        "target_encoding_performance['logistic_regression'].update({'precision':sum(lr_pre)/len(lr_pre)})\r\n",
        "target_encoding_performance['xgboost'].update({'precision':sum(xgb_pre)/len(xgb_pre)})\r\n",
        "target_encoding_performance['nn'].update({'precision':sum(nn_pre)/len(nn_pre)})\r\n",
        "target_encoding_performance['logistic_regression'].update({'recall':sum(lr_rc)/len(lr_rc)})\r\n",
        "target_encoding_performance['xgboost'].update({'recall':sum(xgb_rc)/len(xgb_rc)})\r\n",
        "target_encoding_performance['nn'].update({'recall':sum(nn_rc)/len(nn_rc)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfwBS4qM5maJ"
      },
      "source": [
        "target_encoding_performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgKQ07L_5mXR"
      },
      "source": [
        "#Convert the target encoding performance to dataframe\r\n",
        "target_encoding_performance_df = pd.DataFrame(data=target_encoding_performance, index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "R_fKYx3q5mUr",
        "outputId": "89cad2d6-afdf-472f-c7c2-fdab6e46dc71"
      },
      "source": [
        "target_encoding_performance_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>logistic_regression</th>\n",
              "      <th>xgboost</th>\n",
              "      <th>nn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>auc</th>\n",
              "      <td>0.671586</td>\n",
              "      <td>0.616630</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.615595</td>\n",
              "      <td>0.602653</td>\n",
              "      <td>0.149023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.664492</td>\n",
              "      <td>0.683720</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           logistic_regression   xgboost        nn\n",
              "auc                   0.671586  0.616630  0.500000\n",
              "precision             0.615595  0.602653  0.149023\n",
              "recall                0.664492  0.683720  0.300000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7oJAgTL3POf"
      },
      "source": [
        "Surprisingly, Neural Network performed worse, xgboost maintains the same results while Logistic Regression changes are quite negligible. Definitely, Target encoding will not be used going forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Jl8Vqg5mRv"
      },
      "source": [
        "#Save the data in csv \r\n",
        "target_encoding_performance_df.to_csv('/content/drive/MyDrive/prediction/target_encoding_performance.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEHaZR193Qvi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}