{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B8hOeG2U3Bf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwuatSTJNHxH"
      },
      "source": [
        "#Import the libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model, metrics, preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMPFNcJaMwO-"
      },
      "source": [
        "#Load the data - The datsset used for the modeling, the fractional data, is loaded directly\n",
        "modeling_dataset = pd.read_csv('/content/drive/MyDrive/prediction/frac_cleaned_fod_data.csv', low_memory = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqjr-Uf9Pcg9"
      },
      "source": [
        "#All columns \n",
        "train_features = [tf for tf in modeling_dataset.columns if tf not in ('HasDetections', 'kfold', 'MachineIdentifier')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT5PwHxBM2uL"
      },
      "source": [
        "def xgb_important_features(fold):\n",
        "  \"\"\"\n",
        "  This function will return a list of the features used in training the model in the order of their importance,\n",
        "  starting from the most important.\n",
        "  It takes fold (an int value within the range of 0 to 9) because the dataset used here is already into 10 kfold for\n",
        "  cross validation. It does not need a parameter if this is not the case of your data.\n",
        "  The train_features (features used in training the model) and modeling_dataset (the data used for the training)\n",
        "  are set in global for shared usage by another function.\n",
        "  \"\"\"\n",
        "  for col in train_features:\n",
        "    #Initialize the Label Encoder\n",
        "    lbl = preprocessing.LabelEncoder()\n",
        "    #Fit the label encoder on each of the features\n",
        "    lbl.fit(modeling_dataset[col])\n",
        "    #Transform\n",
        "    modeling_dataset.loc[:,col] = lbl.transform(modeling_dataset[col])\n",
        "    \n",
        "  #Get training and validation data using folds (5 as a dummy value)\n",
        "  modeling_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  modeling_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Get train data - For tree models\n",
        "  X_train = modeling_datasets_train[train_features].values\n",
        "  #Get validation data\n",
        "  X_valid = modeling_datasets_valid[train_features].values\n",
        "\n",
        "  #Initialize XGboost model\n",
        "  xgb_model = xgb.XGBClassifier(n_jobs=-1)\n",
        "  \n",
        "  #Fit the model on training data\n",
        "  xgb_model.fit(X_train, modeling_datasets_train.HasDetections.values)\n",
        "  \n",
        "  #Feature importance\n",
        "  importances = xgb_model.feature_importances_\n",
        "  \n",
        "  idxs = np.argsort(importances)\n",
        "  \n",
        "  return [train_features[i] for i in reversed(idxs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cnXqVC0M2fl"
      },
      "source": [
        "#Function for the Logistic Regression Classifier\n",
        "def run_lr(fold, j):\n",
        "  #Get training and validation data using folds\n",
        "  cleaned_fold_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  cleaned_fold_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [cleaned_fold_datasets_train[train_features][:j],cleaned_fold_datasets_valid[train_features][:j]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features][:j])\n",
        "  \n",
        "  #transform the training and validation data\n",
        "  x_train = ohe.transform(cleaned_fold_datasets_train[train_features][:j])\n",
        "  x_valid = ohe.transform(cleaned_fold_datasets_valid[train_features][:j])\n",
        "\n",
        "  #Initialize the Logistic Regression Model\n",
        "  lr_model = linear_model.LogisticRegression()\n",
        "\n",
        "  #Fit model on training data (ohe)\n",
        "  lr_model.fit(x_train, cleaned_fold_datasets_train.HasDetections.values)\n",
        "\n",
        "  #Predict on the validation data using the probability of 1s for the AUC\n",
        "  valid_preds = lr_model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  #Get the ROC AUC score\n",
        "  auc = metrics.roc_auc_score(cleaned_fold_datasets_valid.HasDetections.values, valid_preds)\n",
        "\n",
        "  return auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENxgVlf3TxeV"
      },
      "source": [
        "def main (fold):\n",
        "  #Initialize the number of iteration\n",
        "  n = 0\n",
        "  #Initialize the index of the items in list_of_features\n",
        "  j = 1\n",
        "  #Calling the features function to return list of the important features according to their importance\n",
        "  list_of_features = xgb_important_features(fold)\n",
        "  #Initialize the lists for number of features and ROC to hold their values after each iteration\n",
        "  no_of_features = []\n",
        "  ROC = []\n",
        "  while (n < len(list_of_features)):\n",
        "    #When n = 0, the first feature is used. So, number of features will be iteration initial value + 1\n",
        "    no_of_features.append(n+1) \n",
        "    # Calling the run_lr function, and its return value, auc, is appended to the ROC list.\n",
        "    ROC.append(run_lr(fold, j))\n",
        "    j +=1\n",
        "    n +=1\n",
        "    continue\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(no_of_features, ROC)\n",
        "  plt.title('The relationship between Number of Features and ROC')\n",
        "  plt.xlabel('Number of Features')\n",
        "  plt.ylabel('ROC')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKfF-XnDTuNu"
      },
      "source": [
        "#Calling the main function \n",
        "main (6)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}