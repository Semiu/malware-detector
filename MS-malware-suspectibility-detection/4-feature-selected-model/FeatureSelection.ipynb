{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureSelection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHUpAtJQsQlE"
      },
      "source": [
        "Two feature selection methods, specifically (a)removing features with low variance and (b) removing features of high correlation are used as attempts to improve the performance of the model.\n",
        "This script shows how the three models (Neural Network, Linear Regression and XGboost) perform features of low variance are removed. The second script, FeatureSelection2, shows the attempt to check for highly correlated features for removal. Only kfold was found to be of correlation greater than 0.95, and it is not used in training at all. Lastly, the third script, FeatureSelection3, shows the ranking of the features importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH18niBfp6SE"
      },
      "source": [
        "#Import the libraries \n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn import ensemble\n",
        "from sklearn import linear_model, metrics, preprocessing\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import xgboost as xgb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n51gptRvr-L-"
      },
      "source": [
        "#Load the data - The datsset used for the modeling, the fractional data, is loaded directly\n",
        "modeling_dataset = pd.read_csv('/content/drive/MyDrive/prediction/frac_cleaned_fod_data.csv', low_memory = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNYCxFFu7tR"
      },
      "source": [
        "#All columns except 'HasDetections', 'kfold', and 'MachineIdentifier' as training features\n",
        "train_features = [tf for tf in modeling_dataset.columns if tf not in ('HasDetections', 'kfold', 'MachineIdentifier')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo15DcZPcqti"
      },
      "source": [
        "#Linear Regression with OHE and Variance Threshold\n",
        "def run_lr_ohe_vt(fold):\n",
        "  #Get the modeling data (train and valid) using fold\n",
        "  modeling_dataset_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  modeling_dataset_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize the OHE and fit it on the whole data\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [modeling_dataset_train[train_features],modeling_dataset_valid[train_features]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features])\n",
        "  #Transform the train and valid data using OHE\n",
        "  x_train = ohe.transform(modeling_dataset_train[train_features])\n",
        "  x_valid = ohe.transform(modeling_dataset_valid[train_features])\n",
        "  #Initialize the variance threshold \n",
        "  variance_threshold = VarianceThreshold(threshold=0.1)\n",
        "  #Transform the x_train and x_valid with the variance threshold\n",
        "  transformed_x_train = variance_threshold.fit_transform(x_train)\n",
        "  transformed_x_valid = variance_threshold.fit_transform(x_valid)\n",
        "  \n",
        "  #Initialize the Logistic Regression Model\n",
        "  lr_model = linear_model.LogisticRegression()\n",
        "\n",
        "  #Fit model on OHE-VT transformed data\n",
        "  lr_model.fit(transformed_x_train, modeling_dataset_train.HasDetections.values)\n",
        "  \n",
        "  #Predict on the validation data using the probability for the AUC\n",
        "  valid_preds = lr_model.predict_proba(transformed_x_valid)[:, 1]\n",
        "\n",
        "  valid_preds_pc = lr_model.predict(transformed_x_valid)\n",
        "  \n",
        "  #Get the ROC AUC score\n",
        "  auc = metrics.roc_auc_score(modeling_dataset_valid.HasDetections.values, valid_preds)\n",
        "\n",
        "  #Get the precision score\n",
        "  pre = metrics.precision_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  #Get the Recall score  \n",
        "  rc = metrics.recall_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  return auc, pre, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoxTYXHeGOzL"
      },
      "source": [
        "#Neural Network with OHE and Variance Threshold\n",
        "def run_nn_ohe_vt(fold):\n",
        "  #Get the modeling data (train and valid) using fold\n",
        "  modeling_dataset_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  modeling_dataset_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize the OHE and fit it on the whole data\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [modeling_dataset_train[train_features],modeling_dataset_valid[train_features]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features])\n",
        "\n",
        "  #Transform the train and valid data using OHE\n",
        "  x_train = ohe.transform(modeling_dataset_train[train_features])\n",
        "  x_valid = ohe.transform(modeling_dataset_valid[train_features])\n",
        "\n",
        "  #Initialize the variance threshold \n",
        "  variance_threshold = VarianceThreshold(threshold=0.1)\n",
        "  #Transform the x_train and x_valid with the variance threshold\n",
        "  transformed_x_train = variance_threshold.fit_transform(x_train)\n",
        "  transformed_x_valid = variance_threshold.fit_transform(x_valid)\n",
        "  \n",
        "  #Initialize the Neural Network Model\n",
        "  nn_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "  #Fit model on OHE-VT transformed data\n",
        "  nn_model.fit(transformed_x_train, modeling_dataset_train.HasDetections.values)\n",
        "  \n",
        "  #Predict on the validation data using the probability for the AUC\n",
        "  valid_preds = nn_model.predict_proba(transformed_x_valid)[:, 1]\n",
        "\n",
        "  valid_preds_pc = nn_model.predict(transformed_x_valid)\n",
        "  \n",
        "  #Get the ROC AUC score\n",
        "  auc = metrics.roc_auc_score(modeling_dataset_valid.HasDetections.values, valid_preds)\n",
        "\n",
        "  #Get the precision score\n",
        "  pre = metrics.precision_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  #Get the Recall score  \n",
        "  rc = metrics.recall_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  return auc, pre, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z37o0Z8GGOsV"
      },
      "source": [
        "#XGBoost with Label Encoder and Variance Threshold\n",
        "def run_xgb_lbl_vt(fold):\n",
        "  #Apply label encoder\n",
        "  for col in train_features:\n",
        "    #Initialize the Label Encoder\n",
        "    lbl = preprocessing.LabelEncoder()\n",
        "    #Fit on the categorical features\n",
        "    lbl.fit(modeling_dataset[col])\n",
        "    #Transform\n",
        "    modeling_dataset.loc[:,col] = lbl.transform(modeling_dataset[col])\n",
        "  \n",
        "  #Get the modeling data (train and valid) using fold\n",
        "  modeling_dataset_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  modeling_dataset_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "    \n",
        "  X_train = modeling_dataset_train[train_features].values\n",
        "  X_valid = modeling_dataset_valid[train_features].values\n",
        "  \n",
        "  #Initialize the variance threshold \n",
        "  variance_threshold = VarianceThreshold(threshold=0.1)\n",
        "\n",
        "  #Transform the x_train and x_valid with the variance threshold\n",
        "  transformed_x_train = variance_threshold.fit_transform(X_train)\n",
        "  transformed_x_valid = variance_threshold.fit_transform(X_valid)\n",
        "  \n",
        "  #Initialize XGboost model\n",
        "  xgb_model = xgb.XGBClassifier(n_jobs=-1)\n",
        "\n",
        "  #Fit model on OHE-VT transformed data\n",
        "  xgb_model.fit(transformed_x_train, modeling_dataset_train.HasDetections.values)\n",
        "  \n",
        "  #Predict on the validation data using the probability for the AUC\n",
        "  valid_preds = xgb_model.predict_proba(transformed_x_valid)[:, 1]\n",
        "\n",
        "  valid_preds_pc = xgb_model.predict(transformed_x_valid)\n",
        "  \n",
        "  #Get the ROC AUC score\n",
        "  auc = metrics.roc_auc_score(modeling_dataset_valid.HasDetections.values, valid_preds)\n",
        "\n",
        "  #Get the precision score\n",
        "  pre = metrics.precision_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  #Get the Recall score  \n",
        "  rc = metrics.recall_score(modeling_dataset_valid.HasDetections.values, valid_preds_pc, average='binary')\n",
        "\n",
        "  return auc, pre, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZaUn3ZnGOmO",
        "outputId": "92f11e63-b067-4e64-e793-133ca7d8a895"
      },
      "source": [
        "lr = []\n",
        "for fold in tqdm(range(10)):\n",
        "  lr.append(run_lr_ohe_vt(fold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 10%|█         | 1/10 [00:36<05:31, 36.84s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 20%|██        | 2/10 [01:20<05:10, 38.75s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 30%|███       | 3/10 [01:53<04:19, 37.07s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 40%|████      | 4/10 [02:25<03:33, 35.55s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 50%|█████     | 5/10 [02:58<02:54, 34.92s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 60%|██████    | 6/10 [03:31<02:17, 34.37s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 70%|███████   | 7/10 [04:05<01:42, 34.14s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 80%|████████  | 8/10 [04:38<01:07, 33.78s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            " 90%|█████████ | 9/10 [05:10<00:33, 33.42s/it]/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "100%|██████████| 10/10 [05:43<00:00, 34.35s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkQZynDtGOjS",
        "outputId": "3c381e06-ce78-4806-e2cd-14b9d3c1eb36"
      },
      "source": [
        "nn = []\n",
        "for fold in tqdm(range(10)):\n",
        "  nn.append(run_nn_ohe_vt(fold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 10%|█         | 1/10 [02:36<23:32, 156.90s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 20%|██        | 2/10 [05:15<20:59, 157.43s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 30%|███       | 3/10 [07:49<18:14, 156.34s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 40%|████      | 4/10 [10:24<15:35, 155.98s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 50%|█████     | 5/10 [13:01<13:00, 156.14s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 60%|██████    | 6/10 [15:38<10:26, 156.52s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 70%|███████   | 7/10 [18:12<07:47, 155.75s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 80%|████████  | 8/10 [20:47<05:11, 155.53s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            " 90%|█████████ | 9/10 [23:24<02:36, 156.04s/it]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "100%|██████████| 10/10 [25:57<00:00, 155.75s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmVwRjPxj0D0",
        "outputId": "97c03f45-33ba-4390-bd31-12c8e2aa5f7c"
      },
      "source": [
        "xg = []\n",
        "for fold in tqdm(range(8)):\n",
        "  xg.append(run_xgb_lbl_vt(fold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [19:05<00:00, 143.23s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-5o9pzpxI11"
      },
      "source": [
        "xg.append(run_xgb_lbl_vt(7))\n",
        "xg.append(run_xgb_lbl_vt(9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1UhKYMVj0Ae"
      },
      "source": [
        "lr_auc = []\n",
        "lr_pre = []\n",
        "lr_rc = []\n",
        "\n",
        "for i in lr:\n",
        "  lr_auc.append(i[0])\n",
        "  lr_pre.append(i[1])\n",
        "  lr_rc.append(i[2])\n",
        "\n",
        "nn_auc = []\n",
        "nn_pre = []\n",
        "nn_rc = []\n",
        "for j in nn:\n",
        "  nn_auc.append(j[0])\n",
        "  nn_pre.append(j[1])\n",
        "  nn_rc.append(j[2])\n",
        "\n",
        "xgb_auc = []\n",
        "xgb_pre = []\n",
        "xgb_rc = []\n",
        "for k in xg:\n",
        "  xgb_auc.append(k[0])\n",
        "  xgb_pre.append(k[1])\n",
        "  xgb_rc.append(k[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY4LMzFQFpH1"
      },
      "source": [
        "#A new dictionary, performance_metrics, updated by the basic model performances metrics, and further\n",
        "#updated by this feature selection (variance threshold) performances\n",
        "performance_metrics = {'basic_model':{'logistic_regression': {'auc': 0.6712106245305044,\n",
        "  'precision': 0.6151031914602935,\n",
        "  'recall': 0.665240386381325},\n",
        " 'nn': {'auc': 0.6730509681316283,\n",
        "  'precision': 0.6171299488566132,\n",
        "  'recall': 0.6588984337245133},\n",
        "  'xgboost': {'auc': 0.6166303840056708,\n",
        "  'precision': 0.6026527561158126,\n",
        "  'recall': 0.6837203542092836 }}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfrqzTiEFpCO"
      },
      "source": [
        "#Update with the Variance Threshold performance metrics\n",
        "performance_metrics.update({'variance':{'logistic_regression':{'auc':sum(lr_auc)/len(lr_auc), 'precision':sum(lr_pre)/len(lr_pre), 'recall':sum(lr_rc)/len(lr_rc)}}})\n",
        "\n",
        "performance_metrics['variance'].update({'nn':{'auc':sum(nn_auc)/len(nn_auc), 'precision':sum(nn_pre)/len(nn_pre), 'recall':sum(nn_rc)/len(nn_rc)}})\n",
        "performance_metrics['variance'].update({'xgboost':{'auc':sum(xgb_auc)/len(xgb_auc), 'precision':sum(xgb_pre)/len(xgb_pre), 'recall':sum(xgb_rc)/len(xgb_rc)}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vloEOjZSaUq6",
        "outputId": "e0c24488-67e3-44e8-ade9-e93249b87227"
      },
      "source": [
        "#Let's see what the dictionary looks like now\n",
        "performance_metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'basic_model': {'logistic_regression': {'auc': 0.6712106245305044,\n",
              "   'precision': 0.6151031914602935,\n",
              "   'recall': 0.665240386381325},\n",
              "  'nn': {'auc': 0.6730509681316283,\n",
              "   'precision': 0.6171299488566132,\n",
              "   'recall': 0.6588984337245133},\n",
              "  'xgboost': {'auc': 0.6166303840056708,\n",
              "   'precision': 0.6026527561158126,\n",
              "   'recall': 0.6837203542092836}},\n",
              " 'variance': {'logistic_regression': {'auc': 0.6324251036951987,\n",
              "   'precision': 0.5920926127654589,\n",
              "   'recall': 0.624090174850011},\n",
              "  'nn': {'auc': 0.6405115252003365,\n",
              "   'precision': 0.6019835134045628,\n",
              "   'recall': 0.6017101005692137},\n",
              "  'xgboost': {'auc': 0.6625380388736228,\n",
              "   'precision': 0.6041411400541387,\n",
              "   'recall': 0.6798264149525999}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k25ErPhrjz6C"
      },
      "source": [
        "#Convert the performance metrics to dataframe\n",
        "performance_metrics_df = pd.DataFrame(data=performance_metrics, index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OQKym_SNikQf",
        "outputId": "5f10293b-0f26-4e84-bbd9-3e9f120c8fae"
      },
      "source": [
        "performance_metrics_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>basic_model</th>\n",
              "      <th>variance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>logistic_regression</th>\n",
              "      <td>{'auc': 0.6712106245305044, 'precision': 0.615...</td>\n",
              "      <td>{'auc': 0.6324251036951987, 'precision': 0.592...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nn</th>\n",
              "      <td>{'auc': 0.6730509681316283, 'precision': 0.617...</td>\n",
              "      <td>{'auc': 0.6405115252003365, 'precision': 0.601...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgboost</th>\n",
              "      <td>{'auc': 0.6166303840056708, 'precision': 0.602...</td>\n",
              "      <td>{'auc': 0.6625380388736228, 'precision': 0.604...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           basic_model                                           variance\n",
              "logistic_regression  {'auc': 0.6712106245305044, 'precision': 0.615...  {'auc': 0.6324251036951987, 'precision': 0.592...\n",
              "nn                   {'auc': 0.6730509681316283, 'precision': 0.617...  {'auc': 0.6405115252003365, 'precision': 0.601...\n",
              "xgboost              {'auc': 0.6166303840056708, 'precision': 0.602...  {'auc': 0.6625380388736228, 'precision': 0.604..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evKp_ufli1l2"
      },
      "source": [
        "variance = {\n",
        "    'logistic_regression': {'auc': 0.6324251036951987,\n",
        "   'precision': 0.5920926127654589,\n",
        "   'recall': 0.624090174850011},\n",
        "  'nn': {'auc': 0.6405115252003365,\n",
        "   'precision': 0.6019835134045628,\n",
        "   'recall': 0.6017101005692137},\n",
        "  'xgboost': {'auc': 0.6625380388736228,\n",
        "   'precision': 0.6041411400541387,\n",
        "   'recall': 0.6798264149525999}\n",
        "   }"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG6Q2NQYO6aj"
      },
      "source": [
        "variance_df = pd.DataFrame(data=variance, index=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Sr6fxC0yF-5l",
        "outputId": "982026a0-e88d-4bc7-c524-015ccaa77575"
      },
      "source": [
        "variance_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>logistic_regression</th>\n",
              "      <th>nn</th>\n",
              "      <th>xgboost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>auc</th>\n",
              "      <td>0.632425</td>\n",
              "      <td>0.640512</td>\n",
              "      <td>0.662538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.592093</td>\n",
              "      <td>0.601984</td>\n",
              "      <td>0.604141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.624090</td>\n",
              "      <td>0.601710</td>\n",
              "      <td>0.679826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           logistic_regression        nn   xgboost\n",
              "auc                   0.632425  0.640512  0.662538\n",
              "precision             0.592093  0.601984  0.604141\n",
              "recall                0.624090  0.601710  0.679826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuqfMFnKnbm"
      },
      "source": [
        "So far, there have not been a significant improvement from the basic model performance, in spite of the feature engineering and feature selections methods applied so far. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAe6FroDGNOC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}