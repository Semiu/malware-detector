{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomizedSearchOpt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3RMBbtHhcqb"
      },
      "source": [
        "The Randomized search method to randomly find the best parameter fit for the optimal accuracy is implemented for each of XGBoost, Logistics Regression and Neural Network. The 43 best features selected are used as the training features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9gEf7LozL5"
      },
      "source": [
        "#Import the libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn import linear_model, metrics, preprocessing, model_selection\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xka9Cq6zqLH1"
      },
      "source": [
        "#Load the data\n",
        "modeling_dataset = pd.read_csv('/content/drive/MyDrive/prediction/frac_cleaned_fod_data.csv', low_memory = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u46bgdwPqK86"
      },
      "source": [
        "#All columns -  except 'HasDetections', 'kfold', and 'MachineIdentifier'\n",
        "train_features = [tf for tf in modeling_dataset.columns if tf not in ('HasDetections', 'kfold', 'MachineIdentifier')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwH-wwKeqKyj"
      },
      "source": [
        "#The features selected based on the feature selection method earlier employed\n",
        "train_features_after_selection = ['AVProductStatesIdentifier', 'Processor','AvSigVersion', 'Census_TotalPhysicalRAM', 'Census_InternalPrimaryDiagonalDisplaySizeInInches', \n",
        "                                  'Census_IsVirtualDevice', 'Census_PrimaryDiskTotalCapacity', 'Wdft_IsGamer', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'EngineVersion',\n",
        "                                  'Census_ProcessorCoreCount', 'Census_OSEdition', 'Census_OSInstallTypeName', 'Census_OSSkuName', 'AppVersion', 'OsBuildLab', 'OsSuite',\n",
        "                                  'Firewall', 'IsProtected', 'Census_IsTouchEnabled', 'Census_ActivationChannel', 'LocaleEnglishNameIdentifier','Census_SystemVolumeTotalCapacity',\n",
        "                                  'Census_InternalPrimaryDisplayResolutionHorizontal','Census_HasOpticalDiskDrive', 'OsBuild', 'Census_InternalPrimaryDisplayResolutionVertical',\n",
        "                                  'CountryIdentifier', 'Census_MDC2FormFactor', 'GeoNameIdentifier', 'Census_PowerPlatformRoleName', 'Census_OSWUAutoUpdateOptionsName', 'SkuEdition',\n",
        "                                  'Census_OSVersion', 'Census_GenuineStateName', 'Census_OSBuildRevision', 'Platform', 'Census_ChassisTypeName', 'Census_FlightRing', \n",
        "                                  'Census_PrimaryDiskTypeName', 'Census_OSBranch', 'Census_IsSecureBootEnabled', 'OsPlatformSubRelease']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_oKejvI4lp1"
      },
      "source": [
        "#Define the categorical features of the data\n",
        "categorical_features = ['ProductName',\n",
        "                        'EngineVersion',\n",
        "                        'AppVersion',\n",
        "                        'AvSigVersion',\n",
        "                        'Platform',\n",
        "                        'Processor',\n",
        "                        'OsVer',\n",
        "                        'OsPlatformSubRelease',\n",
        "                        'OsBuildLab',\n",
        "                        'SkuEdition',\n",
        "                        'Census_MDC2FormFactor',\n",
        "                        'Census_DeviceFamily',\n",
        "                        'Census_PrimaryDiskTypeName',\n",
        "                        'Census_ChassisTypeName',\n",
        "                        'Census_PowerPlatformRoleName',\n",
        "                        'Census_OSVersion',\n",
        "                        'Census_OSArchitecture',\n",
        "                        'Census_OSBranch',\n",
        "                        'Census_OSEdition',\n",
        "                        'Census_OSSkuName',\n",
        "                        'Census_OSInstallTypeName',\n",
        "                        'Census_OSWUAutoUpdateOptionsName',\n",
        "                        'Census_GenuineStateName',\n",
        "                        'Census_ActivationChannel',\n",
        "                        'Census_FlightRing']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUJs2LHsX3LR"
      },
      "source": [
        "#Defining optimization function for the XGBoost classifier\n",
        "def optimize_xgb(fold):\n",
        "  #Apply Label encoder to the categorical features\n",
        "  for feature in train_features:\n",
        "    if feature in categorical_features:\n",
        "      #Initialize the Label Encoder\n",
        "      lbl = preprocessing.LabelEncoder()\n",
        "      #Fit the label encoder on each of the features\n",
        "      lbl.fit(modeling_dataset[feature])\n",
        "      #Transform \n",
        "      modeling_dataset.loc[:,feature] = lbl.transform(modeling_dataset[feature])\n",
        "  \n",
        "  #Get training and validation data using the fold\n",
        "  modeling_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  modeling_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Get the data for modeling - Selected features\n",
        "  modeling_dataset_features_train = modeling_datasets_train[train_features_after_selection].values\n",
        "  modeling_dataset_features_valid = modeling_datasets_valid[train_features_after_selection].values\n",
        "\n",
        "  #Standardizing the data because this is a Linear model\n",
        "  sc = StandardScaler()\n",
        "\n",
        "  scaled_modeling_datasets_train = sc.fit_transform(modeling_dataset_features_train)\n",
        "  scaled_modeling_datasets_valid = sc.transform(modeling_dataset_features_valid)\n",
        "\n",
        "  #Initialize the XGBoost classifier\n",
        "  xgb_model = xgb.XGBClassifier(n_jobs=-1)\n",
        "  #Define grid of parameters\n",
        "  param_grid_xgb = {\n",
        "    'eta': [0.01, 0.015, 0.025, 0.05, 0.1],\n",
        "    'gamma': [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
        "    'max_depth': [3,5,7,9,12,15,17,25],\n",
        "    'min_child_weight': [1,3,5,7],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'lamda': [0.01, 0.05, 0.1, 1.0],\n",
        "    'alpha': [0, 0.1, 0.5, 1.0]\n",
        "    }\n",
        "    #Initialize RandomSearch \n",
        "  model = model_selection.RandomizedSearchCV(\n",
        "      estimator = xgb_model,\n",
        "      param_distributions = param_grid_xgb,\n",
        "      n_iter = 20,\n",
        "      scoring = 'accuracy',\n",
        "      verbose = 10,\n",
        "      n_jobs = 1,\n",
        "      cv = 2\n",
        "      )\n",
        "  #Fit the model and extract the best score\n",
        "  model.fit(scaled_modeling_datasets_train, modeling_datasets_train.HasDetections.values)\n",
        "  print(f\"Best score: {model.best_score_}\")\n",
        "  \n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = model.best_estimator_.get_params()\n",
        "  for param_name in sorted(param_grid_xgb.keys()):\n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94B_4cK4Thy",
        "outputId": "5d344d11-d868-4e2c-bc94-1fef087700fd"
      },
      "source": [
        "#Call the function with any fold number\n",
        "optimize_xgb(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
            "[CV] subsample=1.0, min_child_weight=7, max_depth=7, lamda=1.0, gamma=0.9, eta=0.05, colsample_bytree=0.8, alpha=0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=1.0, min_child_weight=7, max_depth=7, lamda=1.0, gamma=0.9, eta=0.05, colsample_bytree=0.8, alpha=0, score=0.632, total= 1.8min\n",
            "[CV] subsample=1.0, min_child_weight=7, max_depth=7, lamda=1.0, gamma=0.9, eta=0.05, colsample_bytree=0.8, alpha=0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=1.0, min_child_weight=7, max_depth=7, lamda=1.0, gamma=0.9, eta=0.05, colsample_bytree=0.8, alpha=0, score=0.632, total= 1.9min\n",
            "[CV] subsample=0.7, min_child_weight=5, max_depth=3, lamda=0.01, gamma=0.3, eta=0.05, colsample_bytree=0.7, alpha=0.1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.7, min_child_weight=5, max_depth=3, lamda=0.01, gamma=0.3, eta=0.05, colsample_bytree=0.7, alpha=0.1, score=0.616, total=  46.6s\n",
            "[CV] subsample=0.7, min_child_weight=5, max_depth=3, lamda=0.01, gamma=0.3, eta=0.05, colsample_bytree=0.7, alpha=0.1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.7, min_child_weight=5, max_depth=3, lamda=0.01, gamma=0.3, eta=0.05, colsample_bytree=0.7, alpha=0.1, score=0.617, total=  47.6s\n",
            "[CV] subsample=1.0, min_child_weight=1, max_depth=17, lamda=0.01, gamma=0.3, eta=0.015, colsample_bytree=0.6, alpha=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  5.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=1.0, min_child_weight=1, max_depth=17, lamda=0.01, gamma=0.3, eta=0.015, colsample_bytree=0.6, alpha=1.0, score=0.629, total= 4.9min\n",
            "[CV] subsample=1.0, min_child_weight=1, max_depth=17, lamda=0.01, gamma=0.3, eta=0.015, colsample_bytree=0.6, alpha=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=1.0, min_child_weight=1, max_depth=17, lamda=0.01, gamma=0.3, eta=0.015, colsample_bytree=0.6, alpha=1.0, score=0.629, total= 5.0min\n",
            "[CV] subsample=0.8, min_child_weight=7, max_depth=3, lamda=0.1, gamma=0.7, eta=0.025, colsample_bytree=0.8, alpha=0.5 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 15.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.8, min_child_weight=7, max_depth=3, lamda=0.1, gamma=0.7, eta=0.025, colsample_bytree=0.8, alpha=0.5, score=0.616, total=  48.2s\n",
            "[CV] subsample=0.8, min_child_weight=7, max_depth=3, lamda=0.1, gamma=0.7, eta=0.025, colsample_bytree=0.8, alpha=0.5 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 15.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.8, min_child_weight=7, max_depth=3, lamda=0.1, gamma=0.7, eta=0.025, colsample_bytree=0.8, alpha=0.5, score=0.617, total=  47.8s\n",
            "[CV] subsample=0.9, min_child_weight=7, max_depth=15, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.9, alpha=0.5 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 16.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.9, min_child_weight=7, max_depth=15, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.9, alpha=0.5, score=0.633, total= 5.4min\n",
            "[CV] subsample=0.9, min_child_weight=7, max_depth=15, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.9, alpha=0.5 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 22.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  subsample=0.9, min_child_weight=7, max_depth=15, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.9, alpha=0.5, score=0.633, total= 5.1min\n",
            "[CV] subsample=0.6, min_child_weight=3, max_depth=7, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.8, alpha=0 \n",
            "[CV]  subsample=0.6, min_child_weight=3, max_depth=7, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.8, alpha=0, score=0.632, total= 2.0min\n",
            "[CV] subsample=0.6, min_child_weight=3, max_depth=7, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.8, alpha=0 \n",
            "[CV]  subsample=0.6, min_child_weight=3, max_depth=7, lamda=0.1, gamma=1.0, eta=0.015, colsample_bytree=0.8, alpha=0, score=0.631, total= 2.0min\n",
            "[CV] subsample=0.8, min_child_weight=3, max_depth=15, lamda=0.01, gamma=0.7, eta=0.025, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=0.8, min_child_weight=3, max_depth=15, lamda=0.01, gamma=0.7, eta=0.025, colsample_bytree=0.6, alpha=1.0, score=0.632, total= 4.2min\n",
            "[CV] subsample=0.8, min_child_weight=3, max_depth=15, lamda=0.01, gamma=0.7, eta=0.025, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=0.8, min_child_weight=3, max_depth=15, lamda=0.01, gamma=0.7, eta=0.025, colsample_bytree=0.6, alpha=1.0, score=0.632, total= 4.3min\n",
            "[CV] subsample=0.7, min_child_weight=5, max_depth=9, lamda=1.0, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=0.7, min_child_weight=5, max_depth=9, lamda=1.0, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0, score=0.635, total= 2.2min\n",
            "[CV] subsample=0.7, min_child_weight=5, max_depth=9, lamda=1.0, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=0.7, min_child_weight=5, max_depth=9, lamda=1.0, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0, score=0.635, total= 2.2min\n",
            "[CV] subsample=1.0, min_child_weight=1, max_depth=15, lamda=1.0, gamma=1.0, eta=0.025, colsample_bytree=0.6, alpha=0.1 \n",
            "[CV]  subsample=1.0, min_child_weight=1, max_depth=15, lamda=1.0, gamma=1.0, eta=0.025, colsample_bytree=0.6, alpha=0.1, score=0.631, total= 4.0min\n",
            "[CV] subsample=1.0, min_child_weight=1, max_depth=15, lamda=1.0, gamma=1.0, eta=0.025, colsample_bytree=0.6, alpha=0.1 \n",
            "[CV]  subsample=1.0, min_child_weight=1, max_depth=15, lamda=1.0, gamma=1.0, eta=0.025, colsample_bytree=0.6, alpha=0.1, score=0.632, total= 3.9min\n",
            "[CV] subsample=0.9, min_child_weight=3, max_depth=3, lamda=0.01, gamma=0.3, eta=0.1, colsample_bytree=0.6, alpha=0.5 \n",
            "[CV]  subsample=0.9, min_child_weight=3, max_depth=3, lamda=0.01, gamma=0.3, eta=0.1, colsample_bytree=0.6, alpha=0.5, score=0.615, total=  37.8s\n",
            "[CV] subsample=0.9, min_child_weight=3, max_depth=3, lamda=0.01, gamma=0.3, eta=0.1, colsample_bytree=0.6, alpha=0.5 \n",
            "[CV]  subsample=0.9, min_child_weight=3, max_depth=3, lamda=0.01, gamma=0.3, eta=0.1, colsample_bytree=0.6, alpha=0.5, score=0.617, total=  37.3s\n",
            "[CV] subsample=0.6, min_child_weight=3, max_depth=15, lamda=0.05, gamma=0.7, eta=0.015, colsample_bytree=1.0, alpha=0.1 \n",
            "[CV]  subsample=0.6, min_child_weight=3, max_depth=15, lamda=0.05, gamma=0.7, eta=0.015, colsample_bytree=1.0, alpha=0.1, score=0.628, total= 6.6min\n",
            "[CV] subsample=0.6, min_child_weight=3, max_depth=15, lamda=0.05, gamma=0.7, eta=0.015, colsample_bytree=1.0, alpha=0.1 \n",
            "[CV]  subsample=0.6, min_child_weight=3, max_depth=15, lamda=0.05, gamma=0.7, eta=0.015, colsample_bytree=1.0, alpha=0.1, score=0.628, total= 6.4min\n",
            "[CV] subsample=0.7, min_child_weight=7, max_depth=9, lamda=0.05, gamma=0.05, eta=0.015, colsample_bytree=0.8, alpha=0.1 \n",
            "[CV]  subsample=0.7, min_child_weight=7, max_depth=9, lamda=0.05, gamma=0.05, eta=0.015, colsample_bytree=0.8, alpha=0.1, score=0.634, total= 2.7min\n",
            "[CV] subsample=0.7, min_child_weight=7, max_depth=9, lamda=0.05, gamma=0.05, eta=0.015, colsample_bytree=0.8, alpha=0.1 \n",
            "[CV]  subsample=0.7, min_child_weight=7, max_depth=9, lamda=0.05, gamma=0.05, eta=0.015, colsample_bytree=0.8, alpha=0.1, score=0.634, total= 2.6min\n",
            "[CV] subsample=0.7, min_child_weight=7, max_depth=3, lamda=0.05, gamma=0.05, eta=0.1, colsample_bytree=0.9, alpha=0.5 \n",
            "[CV]  subsample=0.7, min_child_weight=7, max_depth=3, lamda=0.05, gamma=0.05, eta=0.1, colsample_bytree=0.9, alpha=0.5, score=0.616, total=  50.7s\n",
            "[CV] subsample=0.7, min_child_weight=7, max_depth=3, lamda=0.05, gamma=0.05, eta=0.1, colsample_bytree=0.9, alpha=0.5 \n",
            "[CV]  subsample=0.7, min_child_weight=7, max_depth=3, lamda=0.05, gamma=0.05, eta=0.1, colsample_bytree=0.9, alpha=0.5, score=0.617, total=  51.3s\n",
            "[CV] subsample=0.8, min_child_weight=3, max_depth=17, lamda=1.0, gamma=0.05, eta=0.015, colsample_bytree=0.9, alpha=0.5 \n",
            "[CV]  subsample=0.8, min_child_weight=3, max_depth=17, lamda=1.0, gamma=0.05, eta=0.015, colsample_bytree=0.9, alpha=0.5, score=0.628, total= 6.7min\n",
            "[CV] subsample=0.8, min_child_weight=3, max_depth=17, lamda=1.0, gamma=0.05, eta=0.015, colsample_bytree=0.9, alpha=0.5 \n",
            "[CV]  subsample=0.8, min_child_weight=3, max_depth=17, lamda=1.0, gamma=0.05, eta=0.015, colsample_bytree=0.9, alpha=0.5, score=0.627, total= 6.9min\n",
            "[CV] subsample=0.9, min_child_weight=1, max_depth=25, lamda=0.01, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=1.0 \n",
            "[CV]  subsample=0.9, min_child_weight=1, max_depth=25, lamda=0.01, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=1.0, score=0.620, total= 9.6min\n",
            "[CV] subsample=0.9, min_child_weight=1, max_depth=25, lamda=0.01, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=1.0 \n",
            "[CV]  subsample=0.9, min_child_weight=1, max_depth=25, lamda=0.01, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=1.0, score=0.620, total= 9.5min\n",
            "[CV] subsample=0.6, min_child_weight=5, max_depth=17, lamda=1.0, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=0.5 \n",
            "[CV]  subsample=0.6, min_child_weight=5, max_depth=17, lamda=1.0, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=0.5, score=0.627, total= 5.6min\n",
            "[CV] subsample=0.6, min_child_weight=5, max_depth=17, lamda=1.0, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=0.5 \n",
            "[CV]  subsample=0.6, min_child_weight=5, max_depth=17, lamda=1.0, gamma=1.0, eta=0.05, colsample_bytree=0.7, alpha=0.5, score=0.628, total= 5.7min\n",
            "[CV] subsample=0.9, min_child_weight=3, max_depth=15, lamda=0.1, gamma=0.9, eta=0.015, colsample_bytree=0.9, alpha=0.1 \n",
            "[CV]  subsample=0.9, min_child_weight=3, max_depth=15, lamda=0.1, gamma=0.9, eta=0.015, colsample_bytree=0.9, alpha=0.1, score=0.632, total= 5.3min\n",
            "[CV] subsample=0.9, min_child_weight=3, max_depth=15, lamda=0.1, gamma=0.9, eta=0.015, colsample_bytree=0.9, alpha=0.1 \n",
            "[CV]  subsample=0.9, min_child_weight=3, max_depth=15, lamda=0.1, gamma=0.9, eta=0.015, colsample_bytree=0.9, alpha=0.1, score=0.630, total= 5.3min\n",
            "[CV] subsample=1.0, min_child_weight=5, max_depth=25, lamda=0.05, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=1.0, min_child_weight=5, max_depth=25, lamda=0.05, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0, score=0.624, total= 7.5min\n",
            "[CV] subsample=1.0, min_child_weight=5, max_depth=25, lamda=0.05, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0 \n",
            "[CV]  subsample=1.0, min_child_weight=5, max_depth=25, lamda=0.05, gamma=0.1, eta=0.05, colsample_bytree=0.6, alpha=1.0, score=0.624, total= 7.4min\n",
            "[CV] subsample=0.8, min_child_weight=5, max_depth=5, lamda=0.1, gamma=0.5, eta=0.1, colsample_bytree=1.0, alpha=0.5 \n",
            "[CV]  subsample=0.8, min_child_weight=5, max_depth=5, lamda=0.1, gamma=0.5, eta=0.1, colsample_bytree=1.0, alpha=0.5, score=0.626, total= 1.5min\n",
            "[CV] subsample=0.8, min_child_weight=5, max_depth=5, lamda=0.1, gamma=0.5, eta=0.1, colsample_bytree=1.0, alpha=0.5 \n",
            "[CV]  subsample=0.8, min_child_weight=5, max_depth=5, lamda=0.1, gamma=0.5, eta=0.1, colsample_bytree=1.0, alpha=0.5, score=0.626, total= 1.5min\n",
            "[CV] subsample=0.8, min_child_weight=1, max_depth=9, lamda=0.01, gamma=0.7, eta=0.1, colsample_bytree=0.9, alpha=0.1 \n",
            "[CV]  subsample=0.8, min_child_weight=1, max_depth=9, lamda=0.01, gamma=0.7, eta=0.1, colsample_bytree=0.9, alpha=0.1, score=0.634, total= 2.8min\n",
            "[CV] subsample=0.8, min_child_weight=1, max_depth=9, lamda=0.01, gamma=0.7, eta=0.1, colsample_bytree=0.9, alpha=0.1 \n",
            "[CV]  subsample=0.8, min_child_weight=1, max_depth=9, lamda=0.01, gamma=0.7, eta=0.1, colsample_bytree=0.9, alpha=0.1, score=0.634, total= 2.9min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 151.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.6345713466977312\n",
            "Best parameters set:\n",
            "\talpha: 1.0\n",
            "\tcolsample_bytree: 0.6\n",
            "\teta: 0.05\n",
            "\tgamma: 0.1\n",
            "\tlamda: 1.0\n",
            "\tmax_depth: 9\n",
            "\tmin_child_weight: 5\n",
            "\tsubsample: 0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTXl_s-at9sL"
      },
      "source": [
        "#Defining optimization function for the Logistics Regression classifier\n",
        "def optimize_lr(fold):\n",
        "  #Get training and validation data using folds\n",
        "  cleaned_fold_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  cleaned_fold_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [cleaned_fold_datasets_train[train_features_after_selection],cleaned_fold_datasets_valid[train_features_after_selection]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features_after_selection])\n",
        "  \n",
        "  #transform the training and validation data\n",
        "  x_train = ohe.transform(cleaned_fold_datasets_train[train_features_after_selection])\n",
        "  x_valid = ohe.transform(cleaned_fold_datasets_valid[train_features_after_selection])\n",
        "\n",
        "  #Initialize the Logistic Regression classifier\n",
        "  lr_model = linear_model.LogisticRegression(max_iter=100)\n",
        "  #Define grid of parameters\n",
        "  param_grid_lr = {\n",
        "      'penalty':['l1', 'l2'],\n",
        "      'C': np.arange(0.001, 100),\n",
        "      'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
        "      }\n",
        "  #Initialize RandomSearch\n",
        "  model = model_selection.RandomizedSearchCV(\n",
        "      estimator = lr_model,\n",
        "      param_distributions = param_grid_lr,\n",
        "      n_iter = 20,\n",
        "      scoring = 'accuracy',\n",
        "      verbose = 10,\n",
        "      n_jobs = 1,\n",
        "      cv = 2\n",
        "      )\n",
        "  #Fit the model and extract the best score\n",
        "  model.fit(x_train, cleaned_fold_datasets_train.HasDetections.values)\n",
        "  print(f\"Best score: {model.best_score_}\")\n",
        "  \n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = model.best_estimator_.get_params()\n",
        "  for param_name in sorted(param_grid_lr.keys()):\n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIApCOVRaP5v"
      },
      "source": [
        "#Function corrected based on error feedback below - then compare the findings\n",
        "optimize_lr(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LuVFj8C6Rj9",
        "outputId": "5aac40b6-0890-471a-d9bb-7bb690d5adc0"
      },
      "source": [
        "#Calling the optimization function with any fold number\n",
        "optimize_lr(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
            "[CV] solver=newton-cg, penalty=l2, C=27.000999999999998 ..............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  \"number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=newton-cg, penalty=l2, C=27.000999999999998, score=0.595, total=35.7min\n",
            "[CV] solver=newton-cg, penalty=l2, C=27.000999999999998 ..............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 35.7min remaining:    0.0s\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  \"number of iterations.\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 71.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=newton-cg, penalty=l2, C=27.000999999999998, score=0.596, total=35.5min\n",
            "[CV] solver=saga, penalty=l2, C=4.0009999999999994 ...................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 72.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=saga, penalty=l2, C=4.0009999999999994, score=0.604, total= 1.1min\n",
            "[CV] solver=saga, penalty=l2, C=4.0009999999999994 ...................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 73.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=saga, penalty=l2, C=4.0009999999999994, score=0.605, total= 1.1min\n",
            "[CV] solver=saga, penalty=l1, C=63.00099999999999 ....................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3JRIkKEvZNm"
      },
      "source": [
        "#Defining optimization function for the Logistics Regression classifier - With Scaler\n",
        "def optimize_lr_scaler(fold):\n",
        "  #Get training and validation data using folds\n",
        "  cleaned_fold_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  cleaned_fold_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [cleaned_fold_datasets_train[train_features_after_selection],cleaned_fold_datasets_valid[train_features_after_selection]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features_after_selection])\n",
        "  \n",
        "  #transform the training and validation data\n",
        "  x_train = ohe.transform(cleaned_fold_datasets_train[train_features_after_selection])\n",
        "  x_valid = ohe.transform(cleaned_fold_datasets_valid[train_features_after_selection])\n",
        "\n",
        "  #Standardizing the data because this is a Linear model\n",
        "  sc = StandardScaler(with_mean=False)\n",
        "\n",
        "  scaled_modeling_datasets_train = sc.fit_transform(x_train)\n",
        "  scaled_modeling_datasets_valid = sc.transform(x_valid)\n",
        "\n",
        "  #Initialize the Logistic Regression classifier\n",
        "  lr_model = linear_model.LogisticRegression(max_iter=100)\n",
        "  #Define grid of parameters\n",
        "  param_grid_lr = {\n",
        "      'penalty':['l1', 'l2'],\n",
        "      'C': np.arange(0.001, 100),\n",
        "      'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
        "      }\n",
        "  #Initialize RandomSearch\n",
        "  model = model_selection.RandomizedSearchCV(\n",
        "      estimator = lr_model,\n",
        "      param_distributions = param_grid_lr,\n",
        "      n_iter = 20,\n",
        "      scoring = 'accuracy',\n",
        "      verbose = 10,\n",
        "      n_jobs = 1,\n",
        "      cv = 2\n",
        "      )\n",
        "  #Fit the model and extract the best score\n",
        "  model.fit(scaled_modeling_datasets_train, cleaned_fold_datasets_train.HasDetections.values)\n",
        "  print(f\"Best score: {model.best_score_}\")\n",
        "  \n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = model.best_estimator_.get_params()\n",
        "  for param_name in sorted(param_grid_lr.keys()):\n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47KtEw1c3KUJ",
        "outputId": "2cf19344-e1a3-46fb-fc46-3025facc26ed"
      },
      "source": [
        "#Call the function\n",
        "optimize_lr_scaler(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
            "[CV] solver=liblinear, penalty=l2, C=40.00099999999999 ...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=liblinear, penalty=l2, C=40.00099999999999, score=0.592, total=10.6min\n",
            "[CV] solver=liblinear, penalty=l2, C=40.00099999999999 ...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 10.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=liblinear, penalty=l2, C=40.00099999999999, score=0.592, total= 9.5min\n",
            "[CV] solver=newton-cg, penalty=l2, C=77.00099999999999 ...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 20.2min remaining:    0.0s\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  \"number of iterations.\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 46.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=newton-cg, penalty=l2, C=77.00099999999999, score=0.591, total=26.2min\n",
            "[CV] solver=newton-cg, penalty=l2, C=77.00099999999999 ...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  \"number of iterations.\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 73.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=newton-cg, penalty=l2, C=77.00099999999999, score=0.592, total=27.3min\n",
            "[CV] solver=liblinear, penalty=l1, C=84.00099999999999 ...............\n",
            "[CV]  solver=liblinear, penalty=l1, C=84.00099999999999, score=0.591, total= 3.8min\n",
            "[CV] solver=liblinear, penalty=l1, C=84.00099999999999 ...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 77.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=liblinear, penalty=l1, C=84.00099999999999, score=0.593, total= 1.3min\n",
            "[CV] solver=saga, penalty=l2, C=70.00099999999999 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 78.8min remaining:    0.0s\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 79.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=saga, penalty=l2, C=70.00099999999999, score=0.594, total=  55.4s\n",
            "[CV] solver=saga, penalty=l2, C=70.00099999999999 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 80.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=saga, penalty=l2, C=70.00099999999999, score=0.595, total=  56.2s\n",
            "[CV] solver=sag, penalty=l1, C=16.000999999999998 ....................\n",
            "[CV]  solver=sag, penalty=l1, C=16.000999999999998, score=nan, total=   0.1s\n",
            "[CV] solver=sag, penalty=l1, C=16.000999999999998 ....................\n",
            "[CV]  solver=sag, penalty=l1, C=16.000999999999998, score=nan, total=   0.1s\n",
            "[CV] solver=saga, penalty=l1, C=92.00099999999999 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 80.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKg-SiUavuYG"
      },
      "source": [
        "#Defining the optimization function for MLP Neural Network Classifier\n",
        "def optimize_nn(fold):\n",
        "  #Get training and validation data using folds\n",
        "  cleaned_fold_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  cleaned_fold_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [cleaned_fold_datasets_train[train_features_after_selection],cleaned_fold_datasets_valid[train_features_after_selection]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features_after_selection])\n",
        "  \n",
        "  #transform the training and validation data\n",
        "  x_train = ohe.transform(cleaned_fold_datasets_train[train_features_after_selection])\n",
        "  x_valid = ohe.transform(cleaned_fold_datasets_valid[train_features_after_selection])\n",
        "\n",
        "  #Initialize the Neural Network classifier\n",
        "  nn_model = MLPClassifier(max_iter=100)\n",
        "  #Define grid of parameters\n",
        "  param_grid_nn = {\n",
        "      'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "      'activation': ['tanh', 'relu'],\n",
        "      'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "      'alpha': [0.001, 0.05],\n",
        "      'learning_rate': ['constant', 'adaptive']\n",
        "      }\n",
        "  #Initialize RandomSearch\n",
        "  model = model_selection.RandomizedSearchCV(\n",
        "      estimator = nn_model,\n",
        "      param_distributions = param_grid_nn,\n",
        "      scoring = 'accuracy',\n",
        "      verbose = 10,\n",
        "      n_jobs = 1,\n",
        "      cv = 2\n",
        "      )\n",
        "  #Fit the model and extract the best score\n",
        "  model.fit(x_train, cleaned_fold_datasets_train.HasDetections.values)\n",
        "  print(f\"Best score: {model.best_score_}\")\n",
        "  \n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = model.best_estimator_.get_params()\n",
        "  for param_name in sorted(param_grid_nn.keys()):\n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jM68G-SwAIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9869fc95-3f19-41bd-d694-66adfe7bcf18"
      },
      "source": [
        "#Calling the optimization function with a fold number\n",
        "optimize_nn(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
            "[CV] solver=sgd, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.05, activation=tanh \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ylEE4dGxagg"
      },
      "source": [
        "#Defining the optimization function for MLP Neural Network Classifier - With Scaler\n",
        "def optimize_nn(fold):\n",
        "  #Get training and validation data using folds\n",
        "  cleaned_fold_datasets_train = modeling_dataset[modeling_dataset.kfold != fold].reset_index(drop=True)\n",
        "  cleaned_fold_datasets_valid = modeling_dataset[modeling_dataset.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  #Initialize OneHotEncoder from scikit-learn, and fit it on training and validation features\n",
        "  ohe = preprocessing.OneHotEncoder()\n",
        "  full_data = pd.concat(\n",
        "    [cleaned_fold_datasets_train[train_features_after_selection],cleaned_fold_datasets_valid[train_features_after_selection]],\n",
        "    axis = 0\n",
        "    )\n",
        "  ohe.fit(full_data[train_features_after_selection])\n",
        "  \n",
        "  #transform the training and validation data\n",
        "  x_train = ohe.transform(cleaned_fold_datasets_train[train_features_after_selection])\n",
        "  x_valid = ohe.transform(cleaned_fold_datasets_valid[train_features_after_selection])\n",
        "\n",
        "  #Standardizing the data because this is a Linear model\n",
        "  sc = StandardScaler(with_mean=False)\n",
        "\n",
        "  scaled_modeling_datasets_train = sc.fit_transform(x_train)\n",
        "  scaled_modeling_datasets_valid = sc.transform(x_valid)\n",
        "\n",
        "  #Initialize the Neural Network classifier\n",
        "  nn_model = MLPClassifier(max_iter=100)\n",
        "  #Define grid of parameters\n",
        "  param_grid_nn = {\n",
        "      'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "      'activation': ['tanh', 'relu'],\n",
        "      'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "      'alpha': [0.001, 0.05],\n",
        "      'learning_rate': ['constant', 'adaptive']\n",
        "      }\n",
        "  #Initialize RandomSearch\n",
        "  model = model_selection.RandomizedSearchCV(\n",
        "      estimator = nn_model,\n",
        "      param_distributions = param_grid_nn,\n",
        "      scoring = 'accuracy',\n",
        "      verbose = 10,\n",
        "      n_jobs = 1,\n",
        "      cv = 2\n",
        "      )\n",
        "  #Fit the model and extract the best score\n",
        "  model.fit(scaled_modeling_datasets_train, cleaned_fold_datasets_train.HasDetections.values)\n",
        "  print(f\"Best score: {model.best_score_}\")\n",
        "  \n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = model.best_estimator_.get_params()\n",
        "  for param_name in sorted(param_grid_nn.keys()):\n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAhhCbPdCG1J",
        "outputId": "8c300f81-7b14-432e-89fb-6e1dcda6bed9"
      },
      "source": [
        "optimize_nn(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
            "[CV] solver=sgd, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.001, activation=relu \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  solver=sgd, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.001, activation=relu, score=0.553, total=364.0min\n",
            "[CV] solver=sgd, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.001, activation=relu \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 364.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}