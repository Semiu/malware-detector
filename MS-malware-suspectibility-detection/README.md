### Using Machine Learning Methods to detect Malware Susceptibility in Windows Computers

The objectives of this work are:
1) To develop a detection model for malware susceptibility in windows computers, and
2) To identify the properties of windows computers, and their order of importance, that determine their malware susceptibility.

The entirety of this work is arranged into six (6) folders, numbered in order. 

`1-data-cleaning-EDA` folder contains notebooks. The `MSmalwaredatasetcleaning.ipynb` contains codes that performed different data cleaning processes. The cleaned version of the dataset is saved. The `MSmalwareEDA.ipynb` contains codes that performed exploratory data analysis of the dataset.

`2-basic-model` contains three notebooks; namely, `1-MSmalwareKfold.ipynb`, `2-MSmalwareBasicModel.ipynb`, and `3-MSmalwareBasicmodel2.ipynb`. The first contains code implementing K-fold cross validation of 10 folds. The second contains the baseline implementation of the three linear models (Logistics Regression, Support Vector Machine, and MultiLayer Perceptron Classifier) employed in this work. The third contains the baseline implementation of two tree models (Random Forest and XGBoost) employed in this work.

`3-feature-engineered-model` contains two notebooks which contain the implementations of two feature engineering methods. `1-TargetEncoding.ipynb` and `2-CombiningCategoricalCols` for target encoding and combinatorial function for the categorical features. Currently, only target encoding was succesfully implemented. The combinatorial function resulted in more data that was beyond the capacity of our computational resources.

`4-feature-selected-model` contains four notebooks. The `FeatureSelection.ipynb` contains implementation of a feature selection method: (a) removing features with low variance. The `FeatureSelection2.ipynb` contains the implementation code for removing features of high correlations. `FeatureSelection3.ipynb` contains code implementation of the XGBoost feature importance. It identifies the order of importance of the features, which are windows properties. Lastly, `FeatureSelection4.ipynb` contains the code implementation of training the machine learning models with the selected features. 

`5-hyperparameter-features-selected-model` contains four notebooks which generally contain the code implementations for Randomized Search optimization and Bayessian Gausian optimization methods. These were implemented for the machine learning models. However, the Randomized search optimization in `RandomizedSearchOpt.ipynb` was only succesful for the XGBoost model, and the succesful implementation of the Bayessian Gausian method was for Logistics Regression and this is contained in `BayessianGausOpt2.ipynb`.

`6-final-model` contains three notebooks. `FinalModel.ipynb` contains the code implementation of Logistics Regression and XGBoost using their selected hyperparameters and features. For experimental purpose, all the features are equally used for training the model, using their tuned hyperparameters, as contained in `FinalModel2.ipynb`. The third notebook, `FinalModel3.ipynb`, contains implementation to identify the optimal value of `max_iter` parameter.
